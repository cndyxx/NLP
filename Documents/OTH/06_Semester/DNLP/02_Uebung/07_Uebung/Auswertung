Aufgabe 7.1:
In main.cpp line 44 - 46 wird der Text Zeile für Zeile eingelesen und trainiert.
In train_nGramm in nGramm.cpp werden in line 117 - 120 die Startsymbole und das Endsymbol hinzugefügt.
Text Generierung siehe nGramm.cpp generateText line 159 ff.

Aufgabe 7.2.1:


Aufgabe 7.2.2:
// Alle Dateien sind in dem Ordner "data" zu finden

Trainingsdaten: merkel-de-shorter.txt
Testdaten: merkel-de-shorter.txt 

    Trainingsdaten und Testdaten sind die selben:
        für n = 4:
            Cross Entropy 1.74286
        für n = 5
            Cross Entropy 1.07128
        für n = 6:
            Cross Entropy 0.743843
        für n = 7:
            Cross Entropy 0.513737
        ...
        für n = 12
            Cross Entropy 0.090018

Trainingsdaten: merkel-de-shorter.txt
Testdaten: 02_inputText_testingEntropy.txt

    Trainingsdaten und Testdaten sind unterschiedlich:
        für n = 4:
            Cross Entropy 1.45324
        für n = 5
            Cross Entropy 0.845688
        für n = 6:
            Cross Entropy 0.497134
        für n = 7:
            Cross Entropy 0.26027
        ...
        für n = 12
            Cross Entropy 0.02284

         => Je größer das N desto kleiner ist die Kreuzentropie. Somit ist das Model für ein größeres N besser.
            Voraussetzung: Die Testdaten müssen aus dem Trainingsdaten entnommen werden (90% Trainingsdaten und 10% Testdaten)
                           


Aufgabe 7.3: